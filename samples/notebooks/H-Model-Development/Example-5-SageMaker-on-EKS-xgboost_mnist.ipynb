{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [4]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.130841,
     "end_time": "2021-05-05T15:02:52.199582",
     "exception": false,
     "start_time": "2021-05-05T15:02:52.068741",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Multiclass classification with Amazon SageMaker XGBoost algorithm\n",
    "_**Single machine and distributed training for multiclass classification with Amazon SageMaker XGBoost algorithm**_\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "## Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Prerequisites and Preprocessing](#Prequisites-and-Preprocessing)\n",
    "  1. [Permissions and environment variables](#Permissions-and-environment-variables)\n",
    "  2. [Data ingestion](#Data-ingestion)\n",
    "  3. [Data conversion](#Data-conversion)\n",
    "3. [Training the XGBoost model](#Training-the-XGBoost-model)\n",
    "  1. [Training on a single instance](#Training-on-a-single-instance)\n",
    "  2. [Training on multiple instances](#Training-on-multiple-instances)\n",
    "4. [Set up hosting for the model](#Set-up-hosting-for-the-model)\n",
    "  1. [Import model into hosting](#Import-model-into-hosting)\n",
    "  2. [Create endpoint configuration](#Create-endpoint-configuration)\n",
    "  3. [Create endpoint](#Create-endpoint)\n",
    "5. [Validate the model for use](#Validate-the-model-for-use)\n",
    "\n",
    "---\n",
    "## Introduction\n",
    "\n",
    "\n",
    "This notebook demonstrates the use of Amazon SageMakerâ€™s implementation of the XGBoost algorithm to train and host a multiclass classification model. The MNIST dataset is used for training. It has a training set of 60,000 examples and a test set of 10,000 examples. To illustrate the use of libsvm training data format, we download the dataset and convert it to the libsvm format before training.\n",
    "\n",
    "To get started, we need to set up the environment with a few prerequisites for permissions and configurations.\n",
    "\n",
    "---\n",
    "## Prequisites and Preprocessing\n",
    "This notebook was tested in Amazon SageMaker Studio on a ml.t3.medium instance with Python 3 (Data Science) kernel. \n",
    "\n",
    "### Permissions and environment variables\n",
    "\n",
    "Here we set up the linkage and authentication to AWS services.\n",
    "\n",
    "1. The roles used to give learning and hosting access to your data. See the documentation for how to specify these.\n",
    "2. The S3 buckets that you want to use for training and model data and where the downloaded data is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true,
    "papermill": {
     "duration": 0.953544,
     "end_time": "2021-05-05T15:02:54.577440",
     "exception": false,
     "start_time": "2021-05-05T15:02:53.623896",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import pprint\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import sagemaker\n",
    "from aws_orbit_sdk.common import get_workspace\n",
    "workspace = get_workspace()\n",
    "role = workspace['EksPodRoleArn']\n",
    "pprint.pprint(workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = %env AWS_ORBIT_ENV\n",
    "team_name = %env AWS_ORBIT_TEAM_SPACE\n",
    "user_name = %env USERNAME\n",
    "namespace = %env AWS_ORBIT_USER_SPACE\n",
    "(env_name,team_name, user_name, namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "papermill": {
     "duration": 0.040665,
     "end_time": "2021-05-05T15:02:54.656288",
     "exception": false,
     "start_time": "2021-05-05T15:02:54.615623",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "PAPERMILL_INPUT_PATH = \"/tmp/e1@20210505-15:02.ipynb\"\n",
    "PAPERMILL_OUTPUT_PATH = \"shared/regression/notebooks/H-Model-Development/Example-1-SageMaker-xgboost_mnist/e1@20210505-15:02.ipynb\"\n",
    "PAPERMILL_OUTPUT_DIR_PATH = (\n",
    "    \"shared/regression/notebooks/H-Model-Development/Example-1-SageMaker-xgboost_mnist\"\n",
    ")\n",
    "PAPERMILL_WORKBOOK_NAME = \"e1@20210505-15:02.ipynb\"\n",
    "PAPERMILL_WORK_DIR = \"/home/jovyan/shared/samples/notebooks/H-Model-Development\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_name = %env AWS_ORBIT_ENV\n",
    "env_name = workspace[\"env_name\"]\n",
    "region = workspace[\"region\"]\n",
    "ssm_parameter_name = (f\"/orbit/{env_name}/demo\")\n",
    "ssm_parameter_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.213617,
     "end_time": "2021-05-05T15:02:54.921509",
     "exception": true,
     "start_time": "2021-05-05T15:02:54.707892",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env_name = workspace[\"env_name\"]\n",
    "ssm_parameter_name = (f\"/orbit/{env_name}/demo\")\n",
    "\n",
    "#ssm_parameter_name = \"/orbit/dev-env/demo\"\n",
    "ssm_client = boto3.client(service_name=\"ssm\")\n",
    "demo_json = json.loads(ssm_client.get_parameter(Name=ssm_parameter_name)[\"Parameter\"][\"Value\"])\n",
    "demo_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get demo env bucket name from ssm parameter. \n",
    "# S3 bucket for saving code and model artifacts.\n",
    "bucket = demo_json[\"LakeBucket\"].split(\":\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 ls s3://$bucket/landing/data/sagemaker/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "isConfigCell": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix = \"sagemaker/DEMO-xgboost-multiclass-classification\"\n",
    "# customize to your bucket where you have stored the data\n",
    "bucket_path = f\"s3://{bucket}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets check to see if MNIST is already staged.  If so, we will jus reuse what is staged....who wants to wait for that dataset to by uploaded???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s3_train = f\"{bucket_path}/{prefix}/train/\"\n",
    "s3_validation = f\"{bucket_path}/{prefix}/validation/\"\n",
    "s3_test = f\"{bucket_path}/{prefix}/test/\"\n",
    "skip_mnist_load = False\n",
    "\n",
    "s3_train_chk = !aws s3 ls {s3_train}\n",
    "s3_validation_chk = !aws s3 ls {s3_validation}\n",
    "s3_test_chk = !aws s3 ls {s3_test}\n",
    "\n",
    "print(len(s3_train_chk))\n",
    "print(len(s3_validation_chk))\n",
    "print(len(s3_test_chk))\n",
    "\n",
    "if len(s3_train_chk)>0 and len(s3_validation_chk)>0 and len(s3_test_chk)>0:\n",
    "    skip_mnist_load = True\n",
    "\n",
    "skip_mnist_load\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Data ingestion\n",
    "\n",
    "Next, we read the MNIST dataset [1] from an existing repository into memory, for preprocessing prior to training. It was downloaded from this [link](http://deeplearning.net/data/mnist/mnist.pkl.gz) and stored in `downloaded_data_bucket`. Processing could be done *in situ* by Amazon Athena, Apache Spark in Amazon EMR, Amazon Redshift, etc., assuming the dataset is present in the appropriate location. Then, the next step would be to transfer the data to S3 for use in training. For small datasets, such as this one, reading into memory isn't onerous, though it would be for larger datasets.\n",
    "\n",
    "> [1] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, November 1998."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Data from toolkit bucket. \n",
    "mnist_data_path = f\"s3://{bucket}/landing/data/sagemaker/mnist.pkl.gz\"\n",
    "mnist_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 ls $mnist_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_mnist_load:\n",
    "    !aws s3 cp $mnist_data_path ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import pickle, gzip, numpy, urllib.request, json\n",
    "if not skip_mnist_load:\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Data conversion\n",
    "\n",
    "Since algorithms have particular input and output requirements, converting the dataset is also part of the process that a data scientist goes through prior to initiating training. In this particular case, the data is converted from pickle-ized numpy array to the libsvm format before being uploaded to S3. The hosted implementation of xgboost consumes the libsvm converted data from S3 for training. The following provides functions for data conversions and file upload to S3 and download from S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import struct\n",
    "import io\n",
    "import boto3\n",
    "\n",
    " \n",
    "def to_libsvm(f, labels, values):\n",
    "     f.write(bytes('\\n'.join(\n",
    "         ['{} {}'.format(label, ' '.join(['{}:{}'.format(i + 1, el) for i, el in enumerate(vec)])) for label, vec in\n",
    "          zip(labels, values)]), 'utf-8'))\n",
    "     return f\n",
    "\n",
    "\n",
    "def write_to_s3(fobj, bucket, key):\n",
    "    return (\n",
    "        boto3.Session(region_name=region).resource(\"s3\").Bucket(bucket).Object(key).upload_fileobj(fobj)\n",
    "    )\n",
    "\n",
    "\n",
    "def get_dataset():\n",
    "    import pickle\n",
    "    import gzip\n",
    "\n",
    "    with gzip.open(\"mnist.pkl.gz\", \"rb\") as f:\n",
    "        u = pickle._Unpickler(f)\n",
    "        u.encoding = \"latin1\"\n",
    "        return u.load()\n",
    "\n",
    "\n",
    "def upload_to_s3(partition_name, partition):\n",
    "    labels = [t.tolist() for t in partition[1]]\n",
    "    vectors = [t.tolist() for t in partition[0]]\n",
    "    num_partition = 5  # partition file into 5 parts\n",
    "    partition_bound = int(len(labels) / num_partition)\n",
    "    for i in range(num_partition):\n",
    "        f = io.BytesIO()\n",
    "        to_libsvm(\n",
    "            f,\n",
    "            labels[i * partition_bound : (i + 1) * partition_bound],\n",
    "            vectors[i * partition_bound : (i + 1) * partition_bound],\n",
    "        )\n",
    "        f.seek(0)\n",
    "        key = f\"{prefix}/{partition_name}/examples{str(i)}\"\n",
    "        url = f\"s3://{bucket}/{key}\"\n",
    "        print(f\"Writing to {url}\")\n",
    "        write_to_s3(f, bucket, key)\n",
    "        print(f\"Done writing to {url}\")\n",
    "\n",
    "\n",
    "def download_from_s3(partition_name, number, filename):\n",
    "    key = f\"{prefix}/{partition_name}/examples{number}\"\n",
    "    url = f\"s3://{bucket}/{key}\"\n",
    "    print(f\"Reading from {url}\")\n",
    "    s3 = boto3.resource(\"s3\", region_name=region)\n",
    "    s3.Bucket(bucket).download_file(key, filename)\n",
    "    try:\n",
    "        s3.Bucket(bucket).download_file(key, \"mnist.local.test\")\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response[\"Error\"][\"Code\"] == \"404\":\n",
    "            print(f\"The object does not exist at {url}.\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "\n",
    "def convert_data():\n",
    "    train_set, valid_set, test_set = get_dataset()\n",
    "    partitions = [(\"train\", train_set), (\"validation\", valid_set), (\"test\", test_set)]\n",
    "    for partition_name, partition in partitions:\n",
    "        print(f\"{partition_name}: {partition[0].shape} {partition[1].shape}\")\n",
    "        upload_to_s3(partition_name, partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if not skip_mnist_load:\n",
    "    convert_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, string\n",
    "unique_id = ''.join(random.choices(string.ascii_lowercase + string.digits, k=4))\n",
    "job_yaml = f\"xgboost-mnist-sm-{unique_id}.yaml\"\n",
    "job_name = f\"xgboost-mnist-sm-operator-{unique_id}\"\n",
    "\n",
    "\n",
    "s3_train = f\"{bucket_path}/{prefix}/train/\"\n",
    "s3_validation = f\"{bucket_path}/{prefix}/validation/\"\n",
    "s3_models = f\"{bucket_path}/{prefix}/models/\"\n",
    "\n",
    "\n",
    "output_bucket = s3_models\n",
    "training_data_bucket = s3_train\n",
    "validation_data_bucket = s3_validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_definition= f'''apiVersion: sagemaker.aws.amazon.com/v1\n",
    "kind: TrainingJob\n",
    "metadata:\n",
    "  name: {job_name}\n",
    "spec:\n",
    "  roleArn: {role}  \n",
    "  region: {region}\n",
    "  algorithmSpecification:\n",
    "    trainingImage: 433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest\n",
    "    trainingInputMode: File\n",
    "  outputDataConfig:\n",
    "    s3OutputPath: {output_bucket}\n",
    "  inputDataConfig:\n",
    "    - channelName: train\n",
    "      dataSource:\n",
    "        s3DataSource:\n",
    "          s3DataType: S3Prefix\n",
    "          s3Uri: {training_data_bucket}\n",
    "          s3DataDistributionType: FullyReplicated\n",
    "      contentType: text/csv\n",
    "      compressionType: None\n",
    "    - channelName: validation\n",
    "      dataSource:\n",
    "        s3DataSource:\n",
    "          s3DataType: S3Prefix\n",
    "          s3Uri: {validation_data_bucket}\n",
    "          s3DataDistributionType: FullyReplicated\n",
    "      contentType: text/csv\n",
    "      compressionType: None\n",
    "  resourceConfig:\n",
    "    instanceCount: 1\n",
    "    instanceType: ml.m4.xlarge\n",
    "    volumeSizeInGB: 5\n",
    "  hyperParameters:\n",
    "    - name: max_depth\n",
    "      value: \"5\"\n",
    "    - name: eta\n",
    "      value: \"0.2\"\n",
    "    - name: gamma\n",
    "      value: \"4\"\n",
    "    - name: min_child_weight\n",
    "      value: \"6\"\n",
    "    - name: silent\n",
    "      value: \"0\"\n",
    "    - name: objective\n",
    "      value: multi:softmax\n",
    "    - name: num_class\n",
    "      value: \"10\"\n",
    "    - name: num_round\n",
    "      value: \"10\"\n",
    "  stoppingCondition:\n",
    "    maxRuntimeInSeconds: 86400'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(job_yaml,\"w\")\n",
    "f.write(job_definition)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_job = !kubectl apply -f {job_yaml} -n {team_name}\n",
    "current_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print(job_name)\n",
    "job_status = !kubectl describe trainingjob {job_name} -n {team_name}\n",
    "j_s = job_status.grep(\"Training Job Status\")[0]\n",
    "if 'InProgress'in j_s  or 'SynchronizingK8sJobWithSageMaker' in j_s or 'ReconcilingTrainingJob' in j_s :\n",
    "    while True:\n",
    "        job_status = !kubectl describe trainingjob {job_name} -n {team_name}\n",
    "        j_s = job_status.grep(\"Training Job Status\")[0]\n",
    "        if 'InProgress'in j_s  or 'SynchronizingK8sJobWithSageMaker' in j_s :\n",
    "            print(job_status.grep('Training Job Status'))\n",
    "            time.sleep(10);\n",
    "            continue\n",
    "        else:\n",
    "            break  \n",
    "f_state = !kubectl describe trainingjob {job_name} -n {team_name}\n",
    "f_state_s = f_state.grep(\"Training Job Status\")[0]\n",
    "print(f\"Final State ---> {f_state_s}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do an assert that ther f_state_s contains 'Completed'\n",
    "j_s_final = job_status.grep(\"Training Job Status\")[0]\n",
    "if 'Completed' in j_s_final:\n",
    "    print('Successful MNIST Training')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look a the SMLogs\n",
    "\n",
    "!kubectl smlogs trainingjobs {job_name} -n {team_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Tags",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('.venv': venv)",
   "name": "python3710jvsc74a57bd00b057235627626f57fbc8c705765c1500164cd2c2844b87f4d5072ebf53a5e1f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "papermill": {
   "default_parameters": {},
   "duration": 4.180815,
   "end_time": "2021-05-05T15:02:55.264826",
   "environment_variables": {},
   "exception": true,
   "input_path": "/tmp/e1@20210505-15:02.ipynb",
   "output_path": "shared/regression/notebooks/H-Model-Development/Example-1-SageMaker-xgboost_mnist/e1@20210505-15:02.ipynb",
   "parameters": {
    "PAPERMILL_INPUT_PATH": "/tmp/e1@20210505-15:02.ipynb",
    "PAPERMILL_OUTPUT_DIR_PATH": "shared/regression/notebooks/H-Model-Development/Example-1-SageMaker-xgboost_mnist",
    "PAPERMILL_OUTPUT_PATH": "shared/regression/notebooks/H-Model-Development/Example-1-SageMaker-xgboost_mnist/e1@20210505-15:02.ipynb",
    "PAPERMILL_WORKBOOK_NAME": "e1@20210505-15:02.ipynb",
    "PAPERMILL_WORK_DIR": "/home/jovyan/shared/samples/notebooks/H-Model-Development"
   },
   "start_time": "2021-05-05T15:02:51.084011",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}