{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master test notebook to run all user notebooks\n",
    "    \n",
    "\n",
    "### Author: AWS Professional Services Emerging Technology and Intelligent Platforms Group\n",
    "### Date: January 30 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import aws.utils.notebooks.controller as controller\n",
    "s3 = boto3.client('s3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scratch_bucket': 'datamaker-per-teamspace-datamakerdevelopmentlake-1hjzehfffpu4z',\n",
       " 'notebook_bucket': 'datamaker-test-base-acco-testnotebookbucket29e9e4-aq5pv0f8xo7y',\n",
       " 'role_arn': 'arn:aws:iam::361427996383:role/datamaker_env_development_team_space_lake-user_role',\n",
       " 'user_namespace': 'development-lake-user',\n",
       " 'instance_profile_arn': 'arn:aws:iam::361427996383:instance-profile/datamaker_env_development_team_space_lake-user_role',\n",
       " 'teamspace_security_group': 'sg-0d2bd440632fb8628',\n",
       " 'external_security_groups': '',\n",
       " 'VPCID': 'vpc-072de3156e305f998',\n",
       " 'NotebookOutputBucket': 'datamaker-test-base-acco-testnotebookbucket29e9e4-aq5pv0f8xo7y',\n",
       " 'ComputeSubnet': 'subnet-09e3b2296e54fee22',\n",
       " 'NotebookSubnet': 'subnet-09e3b2296e54fee22',\n",
       " 'region': 'us-west-2',\n",
       " 'env_name': 'development',\n",
       " 'team_space': 'lake-user'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from aws.utils.notebooks.common import get_workspace,get_scratch_database\n",
    "\n",
    "workspace = get_workspace()\n",
    "scratch_glue_db = get_scratch_database()\n",
    "team_space = workspace['team_space']\n",
    "# DO NOT RUN THIS NOTEBOOK IN LAKE CREATOR TEAM SPACE \n",
    "assert team_space == 'lake-user'\n",
    "notebook_bucket = workspace['NotebookOutputBucket']\n",
    "workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkNotebooks(executions, expected_count):\n",
    "    assert len(executions) == expected_count\n",
    "    for index, row in executions.iterrows():\n",
    "        if 'error@' in row['relativePath']:\n",
    "            raise AssertionError('error in ' + row['relativePath'])\n",
    "    print(\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://datamaker-test-base-acco-testnotebookbucket29e9e4-1708al4diogtz/lake-user/output/notebooks/tests/J-FHIR-Development/Example-1-CreateTables/e1@20200327-19:57.ipynb\n",
      "delete: s3://datamaker-test-base-acco-testnotebookbucket29e9e4-1708al4diogtz/lake-user/output/notebooks/tests/J-FHIR-Development/Example-1-CreateTables/error@e1@20200327-19:45.ipynb\n",
      "delete: s3://datamaker-test-base-acco-testnotebookbucket29e9e4-1708al4diogtz/lake-user/output/notebooks/tests/J-FHIR-Development/Example-2-FHIR-Queries/error@e1@20200327-21:03.ipynb\n",
      "delete: s3://datamaker-test-base-acco-testnotebookbucket29e9e4-1708al4diogtz/lake-user/output/notebooks/tests/J-FHIR-Development/Example-2-FHIR-Queries/error@e1@20200327-20:31.ipynb\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!aws s3 ls $notebook_bucket/lake-user/output/notebooks/tests/B-DataAnalyst/ --recursive\n",
    "!aws s3 rm s3://$notebook_bucket/lake-user/output/notebooks/tests/B-DataAnalyst/ --recursive \n",
    "!aws s3 rm s3://$notebook_bucket/lake-user/output/notebooks/tests/J-FHIR-Development/ --recursive \n",
    "!aws s3 rm s3://$notebook_bucket/emr-logs/ --quiet --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aws.utils.notebooks.database import get_redshift\n",
    "from aws.utils.notebooks.common import get_workspace,get_scratch_database\n",
    "rs = get_redshift()\n",
    "workspace = get_workspace()\n",
    "team_space = workspace['team_space']\n",
    "# DO NOT RUN THIS NOTEBOOK IN LAKE CREATOR TEAM SPACE \n",
    "assert team_space == 'lake-user'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets make sure the lake was created properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lake-creator/output/notebooks/tests/A-LakeCreator/Example-3-Load-Database/unsecured-1@20200325-22:24.ipynb',\n",
       " 'lake-creator/output/notebooks/tests/A-LakeCreator/Example-3-Load-Database/unsecured-2@20200325-22:25.ipynb',\n",
       " 'lake-creator/output/notebooks/tests/A-LakeCreator/Example-3-Load-Database/unsecured-3@20200325-22:24.ipynb',\n",
       " 'lake-creator/output/notebooks/tests/A-LakeCreator/Example-3-Load-Database/unsecured-4@20200325-22:25.ipynb',\n",
       " 'lake-creator/output/notebooks/tests/A-LakeCreator/Example-3-Load-Database/unsecured-5@20200325-22:34.ipynb',\n",
       " 'lake-creator/output/notebooks/tests/A-LakeCreator/Example-3-Load-Database/unsecured-6@20200325-22:33.ipynb',\n",
       " 'lake-creator/output/notebooks/tests/A-LakeCreator/Example-3-Load-Database/unsecured-7@20200325-22:33.ipynb']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "notebooks = []\n",
    "for key in s3.list_objects(Bucket=notebook_bucket)['Contents']:\n",
    "    path = key['Key']\n",
    "    if path.startswith('lake-creator/output/notebooks/tests/A-LakeCreator/Example-3-Load-Database/'):\n",
    "        notebooks.append(path)\n",
    "notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(notebooks) == 7\n",
    "for x in notebooks:\n",
    "    if 'error@' in x:\n",
    "        raise AssertionError('error in ' + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A-LakeCreator\tD-DataQuality\t     H-Model-Development\n",
      "B-DataAnalyst\tE-DataMarts\t     J-FHIR-Development\n",
      "C-DataProfling\tF-AdvanceStatistics  Z-Tests\n"
     ]
    }
   ],
   "source": [
    "!ls .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --out output --err error\n",
    "\n",
    "ls ../B-DataAnalyst/*.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cluster_name' in  os.environ.keys():\n",
    "    clusterName = os.environ['cluster_name']\n",
    "else:\n",
    "    clusterName = 'lake-user-TestCluster'\n",
    "\n",
    "print(clusterName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aws.utils.notebooks.spark.emr as sparkConnection\n",
    "\n",
    "(livy_url, cluster_id, started) = sparkConnection.connect_to_spark(clusterName,\n",
    "                                                             reuseCluster=True, \n",
    "                                                             startCluster=True, \n",
    "                                                             clusterArgs={\n",
    "                                                                        \"CoreInstanceCount\":5\n",
    "                                                                  })\n",
    "(livy_url, cluster_id, started) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-27 14:57:56 INFO     Using internal cluster\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%connect_to_redshift -cluster db-test -reuse -start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * redshift+psycopg2://IAMA%3Amaster:***@env-test5-lake-user-db-test.cmbw1wxtyi2p.us-east-1.redshift.amazonaws.com:5439/defaultdb\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(1,)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "\n",
    "SELECT 1 as \"Test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure regression run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebooks_run_config = {\n",
    "    \"black_list\": [],          # a list of notebooks names to skip the execution for. Example: [\"Example-7-Data-Profiling\"]\n",
    "    \"white_list\": [],          # if not empty, only those will run. Example: [\"Example-7-Data-Profiling\"]\n",
    "    \"optional_list\": [],       # indicates to ignoore a failure. Example: [\"Example-6-Schedule-Notebook\", \"Example-8-LakeFormation-Security\"]\n",
    "    \"minimum_successful\": 1,   # number of minimum notebooks to be completed to consider entire test not failed (has an effect when this number is larger than number of mandatory )\n",
    "    \"maxRetries\": 3,           # max number of attempts to execute a notebook\n",
    "    \"notebooks_to_run\": []     # all noootebooks for execution.\n",
    "}\n",
    " \n",
    "for p in output.split('\\n'):\n",
    "    if (len(p)<2):\n",
    "        continue \n",
    "    parts = p.split('/')\n",
    "    nb_name, nb_folder = parts[2][::-1].split('.',1)[1][::-1], parts[1]\n",
    "    if nb_name in notebooks_run_config[\"black_list\"]:\n",
    "        # ignore white list. black list is having highest priority for filters\n",
    "        continue\n",
    "    if not notebooks_run_config[\"white_list\"] or nb_name in notebooks_run_config[\"white_list\"]:\n",
    "        # run notebook if white list is empty or if the notebook is in white list.\n",
    "        notebooks_run_config[\"notebooks_to_run\"].append({\"folder\": nb_folder, \"name\": nb_name})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New implementation of the cell below\n",
    "import time\n",
    "\n",
    "#Prepare noteboooks for a run. Delete previous output files\n",
    "def prepare_notebooks(run_config):\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(notebook_bucket)\n",
    "    for nb in run_config[\"notebooks_to_run\"]:\n",
    "        nb_path = 'lake-user/output/notebooks/tests/B-DataAnalyst/{}/'.format(nb['name'])\n",
    "        bucket.objects.filter(Prefix = nb_path).delete()\n",
    "\n",
    "def start_notebooks(run_config):\n",
    "    _containers = []\n",
    "    for nb in run_config[\"notebooks_to_run\"]:\n",
    "        notebook_to_run = {\n",
    "            \"tasks\": [{\n",
    "                      \"notebookName\": \"{}.ipynb\".format(nb['name']),\n",
    "                      \"sourcePath\": \"samples/notebooks/{}\".format(nb['folder']),\n",
    "                      \"targetPath\": \"tests/{}\".format(nb['folder']),\n",
    "                      \"params\": {\n",
    "                      }\n",
    "                    }],\n",
    "            \"env_vars\": [\n",
    "                    {\n",
    "                        'name': 'cluster_name',\n",
    "                        'value': clusterName\n",
    "                    }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        container = controller.run_notebooks(notebook_to_run)\n",
    "        print(\"notebookName: \" + str(container))\n",
    "        _containers = _containers + container\n",
    "    return _containers\n",
    "\n",
    "def get_execution_results(run_config):\n",
    "    result = {\"success\": [], \"failed\": []}\n",
    "    \n",
    "    for key in s3.list_objects(Bucket=notebook_bucket)['Contents']:\n",
    "        path = key['Key']\n",
    "        if not path.startswith('lake-user/output/notebooks/tests/B-DataAnalyst'):\n",
    "            continue\n",
    "        \n",
    "        nb_name = path.split(\"/\")[-2] # Name of the folder for the output file is a notebook name\n",
    "        res = path.split(\"/\")[-1] # result of notebook actual execution (output)\n",
    "        \n",
    "        # check results only for those that were executed.\n",
    "        nb_names = [ nb[\"name\"] for nb in run_config['notebooks_to_run'] ]\n",
    "        if nb_name not in run_config['notebooks_to_run']:\n",
    "            continue\n",
    "        \n",
    "        if 'error@' in res and 'Failure-Behavior' not in nb_name:\n",
    "            result[\"failed\"].append(nb_name)\n",
    "        elif 'error@' in res and 'Failure-Behavior' in nb_name:\n",
    "            result[\"success\"].append(nb_name)\n",
    "        elif 'error@' not in res and 'Failure-Behavior' in nb_name:\n",
    "            result[\"failed\"].append(nb_name)\n",
    "        elif 'error@' not in res and 'Failure-Behavior' not in nb_name:\n",
    "            result[\"success\"].append(nb_name)\n",
    "        else:\n",
    "            pass #impossible to be here.\n",
    "        \n",
    "    return result\n",
    "\n",
    "def update_run_config(run_config, execution_results):\n",
    "    executed = run_config['notebooks_to_run']\n",
    "    run_config['notebooks_to_run'] = [] #reset notebooks for the next execution\n",
    "    \n",
    "    # if nothing failed\n",
    "    if not execution_results['failed']:\n",
    "        return run_config\n",
    "    \n",
    "    for nb in executed:\n",
    "        if nb['name'] in execution_results['failed']:\n",
    "            run_config['notebooks_to_run'].append(nb)\n",
    "    \n",
    "    return run_config\n",
    "\n",
    "#containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = False\n",
    "attempt = 0\n",
    "run_config = notebooks_run_config\n",
    "containers = []\n",
    "while(attempt < notebooks_run_config[\"maxRetries\"] and not success):\n",
    "    attempt += 1\n",
    "    print(f\"Starting notebooks. Attempt {attempt}. Run config: {run_config}\")\n",
    "    prepare_notebooks(run_config)\n",
    "    containers = start_notebooks(run_config)\n",
    "    controller.wait_for_tasks_to_complete(containers, 120,45, False)\n",
    "    results = get_execution_results(run_config)\n",
    "    print(f'Attemp {attempt} finished. Results: {results}')\n",
    "    run_config = update_run_config(run_config, results)\n",
    "    success = not run_config[\"notebooks_to_run\"]\n",
    "    \n",
    "# After all attempts, check for failed notebooks and assert ONLY if failed is not optional:\n",
    "# Don't mark this notebook failed, Test_Report notebook will decide.\n",
    "#for nb in run_config['notebooks_to_run']:\n",
    "#    if (nb)\n",
    "#    raise AssertionError('error in ' + x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notebookPaths = [] #this is not used anywhere and should be removed. only objections?\n",
    "# containers = []\n",
    "\n",
    "# for p in output.split('\\n'):\n",
    "#     if (len(p)<2):\n",
    "#         continue \n",
    "#     parts = p.split('/')  \n",
    "#     folder = parts[1]\n",
    "#     notebookName = parts[2]  \n",
    "#     notebooksToRun = {\n",
    "#         \"tasks\": [{\n",
    "#                   \"notebookName\": \"{}\".format(notebookName),\n",
    "#                   \"sourcePath\": \"samples/notebooks/{}\".format(folder),\n",
    "#                   \"targetPath\": \"tests/{}\".format(folder),\n",
    "#                   \"params\": {\n",
    "#                   }\n",
    "#                 }],\n",
    "#         \"env_vars\": [\n",
    "#                 {\n",
    "#                     'name': 'cluster_name',\n",
    "#                     'value': clusterName\n",
    "#                 }\n",
    "#         ]\n",
    "#     }\n",
    "    \n",
    "#     container = controller.run_notebooks(notebooksToRun)\n",
    "#     print(\"notebookName: \" + str(container))\n",
    "#     containers = containers + container\n",
    "# containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "controller.wait_for_tasks_to_complete(containers, 120,45, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.get_execution_history(\"samples/notebooks/Z-Tests\", \"run-analyst-notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if started:\n",
    "    sparkConnection.stop_cluster(cluster_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%delete_redshift_cluster -cluster db-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets run FHIR notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "containers = controller.run_notebooks({\n",
    "        \"tasks\":  [ {\n",
    "          \"notebookName\": \"Example-0-Build-Schema-Induction.ipynb\",\n",
    "          \"sourcePath\": \"samples/notebooks/J-FHIR-Development\",\n",
    "          \"targetPath\": \"tests/J-FHIR-Development\"\n",
    "        }]\n",
    "}\n",
    ")\n",
    "containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "controller.wait_for_tasks_to_complete(containers, 60,15, False)\n",
    "executions = controller.get_execution_history(\"tests/J-FHIR-Development\", \"Example-0-Build-Schema-Induction\")  \n",
    "display(executions)\n",
    "#checkNotebooks(executions,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arn:aws:ecs:us-east-1:495869084367:task/8e05332a-5ddc-4e2c-9393-dafd0f9f4a9d']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "containers = controller.run_notebooks({\n",
    "        \"tasks\":  [ {\n",
    "          \"notebookName\": \"Example-1-CreateTables.ipynb\",\n",
    "          \"sourcePath\": \"samples/notebooks/J-FHIR-Development\",\n",
    "          \"targetPath\": \"tests/J-FHIR-Development\"\n",
    "        }]\n",
    "}\n",
    ")\n",
    "containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relativePath</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>s3path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e1@20200327-21:25.ipynb</td>\n",
       "      <td>2020-03-27 21:31:15+00:00</td>\n",
       "      <td>s3://datamaker-test-base-acco-testnotebookbuck...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              relativePath                 timestamp  \\\n",
       "0  e1@20200327-21:25.ipynb 2020-03-27 21:31:15+00:00   \n",
       "\n",
       "                                              s3path  \n",
       "0  s3://datamaker-test-base-acco-testnotebookbuck...  "
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS\n",
      "CPU times: user 185 ms, sys: 2.83 ms, total: 188 ms\n",
      "Wall time: 8min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "controller.wait_for_tasks_to_complete(containers, 60,15, False)\n",
    "executions = controller.get_execution_history(\"tests/J-FHIR-Development\", \"Example-1-CreateTables.ipynb\")  \n",
    "display(executions)\n",
    "#checkNotebooks(executions,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arn:aws:ecs:us-east-1:495869084367:task/14674b10-e029-4aff-b13a-27f49208500f']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "containers = controller.run_notebooks({\n",
    "        \"tasks\":  [ {\n",
    "          \"notebookName\": \"Example-2-FHIR-Queries.ipynb\",\n",
    "          \"sourcePath\": \"samples/notebooks/J-FHIR-Development\",\n",
    "          \"targetPath\": \"tests/J-FHIR-Development\"\n",
    "        }]\n",
    "}\n",
    ")\n",
    "containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relativePath</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>s3path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e1@20200327-21:34.ipynb</td>\n",
       "      <td>2020-03-27 21:38:51+00:00</td>\n",
       "      <td>s3://datamaker-test-base-acco-testnotebookbuck...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              relativePath                 timestamp  \\\n",
       "0  e1@20200327-21:34.ipynb 2020-03-27 21:38:51+00:00   \n",
       "\n",
       "                                              s3path  \n",
       "0  s3://datamaker-test-base-acco-testnotebookbuck...  "
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS\n",
      "CPU times: user 166 ms, sys: 15.1 ms, total: 181 ms\n",
      "Wall time: 8min\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "controller.wait_for_tasks_to_complete(containers, 60,15, False)\n",
    "executions = controller.get_execution_history(\"tests/J-FHIR-Development\", \"Example-2-FHIR-Queries.ipynb\")  \n",
    "display(executions)\n",
    "#checkNotebooks(executions,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of notebook\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
