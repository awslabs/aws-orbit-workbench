{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master test notebook to run all user notebooks\n",
    "    \n",
    "\n",
    "### Author: AWS Professional Services Emerging Technology and Intelligent Platforms Group\n",
    "### Date: Feb 4 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "from aws_orbit_sdk import controller\n",
    "from aws_orbit_sdk.common import get_workspace,get_scratch_database\n",
    "s3 = boto3.client('s3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = get_workspace()\n",
    "scratch_glue_db = get_scratch_database()\n",
    "team_space = workspace['team_space']\n",
    "# DO NOT RUN THIS NOTEBOOK IN LAKE CREATOR TEAM SPACE \n",
    "assert team_space == 'lake-user'\n",
    "scratch_bucket = workspace['scratch-bucket']\n",
    "(team_space, scratch_bucket, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkNotebooks(executions, expected_count):\n",
    "    assert len(executions) == expected_count\n",
    "    for index, row in executions.iterrows():\n",
    "        if 'error@' in row['relativePath']:\n",
    "            raise AssertionError('error in ' + row['relativePath'])\n",
    "    print(\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets make sure the lake was created properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure regression run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --out output --err error\n",
    "\n",
    "ls ../B-DataAnalyst/*.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebooks_run_config = {\n",
    "    \"black_list\": [],          # a list of notebooks names to skip the execution for. Example: [\"Example-7-Data-Profiling\"]\n",
    "    \"white_list\": [],          # if not empty, only those will run. Example: [\"Example-7-Data-Profiling\"]\n",
    "    \"optional_list\": [],       # indicates to ignoore a failure. Example: [\"Example-6-Schedule-Notebook\", \"Example-8-LakeFormation-Security\"]\n",
    "    \"minimum_successful\": 1,   # number of minimum notebooks to be completed to consider entire test not failed (has an effect when this number is larger than number of mandatory )\n",
    "    \"maxRetries\": 3,           # max number of attempts to execute a notebook\n",
    "    \"notebooks_to_run\": []     # all noootebooks for execution.\n",
    "}\n",
    " \n",
    "for p in output.split('\\n'):\n",
    "    if (len(p)<2):\n",
    "        continue \n",
    "    parts = p.split('/')\n",
    "    nb_name, nb_folder = parts[2][::-1].split('.',1)[1][::-1], parts[1]\n",
    "    if nb_name in notebooks_run_config[\"black_list\"]:\n",
    "        # ignore white list. black list is having highest priority for filters\n",
    "        continue\n",
    "    if not notebooks_run_config[\"white_list\"] or nb_name in notebooks_run_config[\"white_list\"]:\n",
    "        # run notebook if white list is empty or if the notebook is in white list.\n",
    "        notebooks_run_config[\"notebooks_to_run\"].append({\"folder\": nb_folder, \"name\": nb_name})\n",
    "notebooks_run_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New implementation of the cell below\n",
    "import time\n",
    "\n",
    "def start_notebooks(run_config):\n",
    "    _containers = []\n",
    "    for nb in run_config[\"notebooks_to_run\"]:\n",
    "        notebook_to_run = {\n",
    "            \"tasks\": [{\n",
    "                      \"notebookName\": \"{}.ipynb\".format(nb['name']),\n",
    "                      \"sourcePath\": f\"shared/samples/notebooks/{nb['folder']}\",\n",
    "                      \"targetPath\": \"shared/regression/notebooks/{}\".format(nb['folder']),\n",
    "                      \"params\": {\n",
    "                      }\n",
    "                    }]\n",
    "        }\n",
    "\n",
    "        container = controller.run_notebooks(notebook_to_run)\n",
    "        print(\"notebookName: \" + str(container))\n",
    "        _containers.append(container)\n",
    "    return _containers\n",
    "\n",
    "\n",
    "def update_run_config(run_config, execution_results):\n",
    "    executed = run_config['notebooks_to_run']\n",
    "    run_config['notebooks_to_run'] = [] #reset notebooks for the next execution\n",
    "    \n",
    "    # if nothing failed\n",
    "    if not execution_results['failed']:\n",
    "        return run_config\n",
    "    \n",
    "    for nb in executed:\n",
    "        if nb['name'] in execution_results['failed']:\n",
    "            run_config['notebooks_to_run'].append(nb)\n",
    "    \n",
    "    return run_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = False\n",
    "attempt = 0\n",
    "run_config = notebooks_run_config\n",
    "containers = []\n",
    "containers = start_notebooks(run_config)\n",
    "controller.wait_for_tasks_to_complete(containers, 120,45, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_execution_results(run_config):\n",
    "    result = {\"success\": [], \"failed\": []}\n",
    "    for test in run_config['notebooks_to_run']:\n",
    "        executions = controller.get_execution_history(f\"shared/regression/notebooks/{test['folder']}\", f\"{test['name']}.ipynb\") \n",
    "        failed = False\n",
    "        passed = False\n",
    "        nb_name = test['name']\n",
    "        for index, row in executions.iterrows():\n",
    "            print(row)\n",
    "            res = row.get('relativePath')\n",
    "            nb_name  = res.split('/')[-2]\n",
    "            print (res)\n",
    "            if 'error@' in res and 'Failure-Behavior' not in nb_name:\n",
    "                failed = True\n",
    "            elif 'error@' in res and 'Failure-Behavior' in nb_name:\n",
    "                passed = True\n",
    "            elif 'error@' not in res and 'Failure-Behavior' in nb_name:\n",
    "                failed = True\n",
    "            elif 'error@' not in res and 'Failure-Behavior' not in nb_name:\n",
    "                passed = True\n",
    "            else:\n",
    "                pass #impossible to be here.\n",
    "        if passed:\n",
    "            result[\"success\"].append(nb_name)\n",
    "        elif failed:\n",
    "            result[\"failed\"].append(nb_name)\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebooks_run_config[\"maxRetries\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "success = False\n",
    "attempt = 0\n",
    "run_config = notebooks_run_config\n",
    "containers = []\n",
    "while(attempt < notebooks_run_config[\"maxRetries\"] and not success):\n",
    "    attempt += 1\n",
    "    print(f\"Starting notebooks. Attempt {attempt}. Run config: {run_config}\")\n",
    "    containers = start_notebooks(run_config)\n",
    "    controller.wait_for_tasks_to_complete(containers, 120,45, False)\n",
    "    results = get_execution_results(run_config)\n",
    "    print(f'Attemp {attempt} finished. Results: {results}')\n",
    "    run_config = update_run_config(run_config, results)\n",
    "    success = not run_config[\"notebooks_to_run\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "controller.wait_for_tasks_to_complete(containers, 120,45, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(results['failed']) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"PASSED\" >> /efs/shared/regression/PASSED\n",
    "!ls /efs/shared/regression/\n",
    "!sleep 15s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of notebook\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}