{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.079672,
     "end_time": "2020-03-03T13:56:37.575779",
     "exception": false,
     "start_time": "2020-03-03T13:56:37.496107",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<!--\n",
    "#  Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "#\n",
    "#    Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "#    You may not use this file except in compliance with the License.\n",
    "#    You may obtain a copy of the License at\n",
    "#\n",
    "#        http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#    Unless required by applicable law or agreed to in writing, software\n",
    "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#    See the License for the specific language governing permissions and\n",
    "#    limitations under the License.\n",
    "-->\n",
    "\n",
    "# Orcherstration notebook for building the lake\n",
    "***End-to-End Orchestration for Building Out a Data Lake in Orbit Workbench***\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Content\n",
    "1. [Introduction](#Orcherstration-notebook-for-building-the-lake)\n",
    "2. [Set Up](#Set-Up)\n",
    " 1. [Imports](#Imports)\n",
    " 2. [Locate Bucket Paths](#Locate-Bucket-Paths)\n",
    " 3. [Create Databases](#Create-Databases)\n",
    " 4. [Get Parameters](#Get-Parameters)\n",
    "3. [S3 Clean Up](#Step-2:-S3-Clean-Up)\n",
    "4. [Extract Zip Files in Parallel](#Step-3:-Extract-Zip-Files-in-Parallel)\n",
    "5. [Read the CSV Files and Create Glue tables with Parquet format according to schema](#Step-4:-Read-the-CSV-Files-and-Create-Glue-tables-with-Parquet-format-according-to-schema)\n",
    " 1. [Connect to Spark and Access Cluster](#Connect-to-Spark-and-Access-Cluster)\n",
    " 2. [Create Glue Tables](#Create-Glue-Tables)\n",
    " 3. [Check Tables are Created](#Check-that-Glue-Tables-are-Created)\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "This notebook orchestrates the Data Lake creation. It performs end-to-end functionality starting with handling zipped csv data files and ultimately creating a data lake on AWS. In order to do so, we execute a set of notebooks for the different steps of creating the lake, including:\n",
    "\n",
    "* **Example-2-Extract-Files** - Extracting Zip Files Data to a Target s3 Bucket in Parallel\n",
    "* **Example-3-Load-Database-Athena** - Find File Schema and Create Glue Tables with Parquet Output\n",
    "\n",
    "To successfully run this notebook, you must be the \"Lake Creator\" user in your environment. Give a look at the following steps for how to go about orchestrating your own data lake build and feel free to look at the other 2 notebooks to get a more in-depth understanding of the process!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports\n",
    "First, let's import all of the modules we will need for building our Data Lake, including Spark EMR Cluster, JSON, etc. Lets store our session state so that we can create service clients to s3 and glue.\n",
    "\n",
    "Next, lets define the location of our notebooks in s3 and check our team space (we **MUST** be the lake-creator to orchestrate our data lake!):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.059109,
     "end_time": "2020-03-03T13:56:37.714349",
     "exception": false,
     "start_time": "2020-03-03T13:56:37.655240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import time\n",
    "import json\n",
    "from aws_orbit_sdk import controller\n",
    "from aws_orbit_sdk.common import get_workspace\n",
    "from pathlib import Path\n",
    "\n",
    "!env | grep AWS\n",
    "\n",
    "# import aws.utils.notebooks.controller as controller\n",
    "# from aws.utils.notebooks.common import get_workspace\n",
    "# import aws.utils.notebooks.spark.emr as sparkConnection\n",
    "my_session = boto3.session.Session()\n",
    "my_region = my_session.region_name\n",
    "s3 = boto3.client('s3')\n",
    "glue = boto3.client('glue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.153377,
     "end_time": "2020-03-03T13:56:38.275433",
     "exception": false,
     "start_time": "2020-03-03T13:56:38.122056",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "workspace = get_workspace()\n",
    "notebook_bucket = workspace['ScratchBucket']\n",
    "team_space = workspace['team_space']\n",
    "env_name = workspace['env_name']\n",
    "workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locate Bucket Paths\n",
    "Now, let's use Amazon Systems Manager (SSM) to get the bucket names for our users, unsecured lake, and secured lake buckets \n",
    "(**Note:** we will use these bucket names to locate and store data later on):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm = boto3.client('ssm')\n",
    "def get_ssm_parameters(ssm_string, ignore_not_found=False):\n",
    "    ssm = boto3.client('ssm')\n",
    "    \n",
    "    try:\n",
    "        return json.loads(ssm.get_parameter(Name=ssm_string)['Parameter']['Value'])\n",
    "    except Exception as e:\n",
    "        if ignore_not_found:\n",
    "            return {}\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "        \n",
    "def get_demo_configuration():\n",
    "    return get_ssm_parameters(f\"/orbit/{env_name}/demo\", True)\n",
    "\n",
    "demo_config = get_demo_configuration()\n",
    "lake_bucket = demo_config.get(\"LakeBucket\").split(':::')[1]\n",
    "users_bucket = notebook_bucket\n",
    "(lake_bucket,users_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Databases\n",
    "We have to create 3 databases: 1 for our raw data, 1 for a default database (both on the lake_bucket path) and 1 for users in our user_bucket.\n",
    "\n",
    "We can do so with the following function which:\n",
    "\n",
    "  **1.** Deletes the existing database with a given name (if it exists)\n",
    "  \n",
    "  **2.** Create new one located in designated s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db(name, location, description=''):\n",
    "    try:\n",
    "        response = glue.delete_database(\n",
    "            Name=name\n",
    "        )\n",
    "    except:\n",
    "        pass\n",
    "    response = glue.create_database(\n",
    "        DatabaseInput={\n",
    "            'Name': name,\n",
    "            'Description': description,\n",
    "            'LocationUri': f's3://{location}/{name}'\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_db('cms_raw_db', lake_bucket,'lake: claims data from cms')\n",
    "create_db('default', lake_bucket)\n",
    "create_db('users', users_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Parameters\n",
    "Lastly, we set the source paths of our Zip files, a path to our extracted data folders, and our database name howing our raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.312929,
     "end_time": "2020-03-03T13:56:38.081259",
     "exception": false,
     "start_time": "2020-03-03T13:56:37.768330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "location = glue.get_database(Name='cms_raw_db')['Database']['LocationUri']\n",
    "bucket = location[5:].split('/')[0]\n",
    "(bucket, location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.060165,
     "end_time": "2020-03-03T13:56:38.402468",
     "exception": false,
     "start_time": "2020-03-03T13:56:38.342303",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "sourcePrefix = \"cms/\"\n",
    "sourceFolder = \"landing/data/\" + sourcePrefix\n",
    "bucketName = bucket\n",
    "extractedPrefix = \"extracted/\"\n",
    "extractedFolder = \"s3://{}/{}\".format(bucketName,extractedPrefix)\n",
    "database_name = \"cms_raw_db\"\n",
    "team_space=workspace['team_space']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042139,
     "end_time": "2020-03-03T13:56:38.676584",
     "exception": false,
     "start_time": "2020-03-03T13:56:38.634445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## Step 2: S3 Clean Up\n",
    "\n",
    "Another step we must take before beginning to extract our data and orchestrate our data lake is to clean out our S3 files for any existing data files sitting there. Let's remove existing content in the following folders:\n",
    "* Files stored in our Extracted Data Folder\n",
    "* Data Stored in our 'cms_raw_db' table\n",
    "* Remove Test Output so we can populate with new test results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.772222,
     "end_time": "2020-03-03T13:56:40.492029",
     "exception": false,
     "start_time": "2020-03-03T13:56:38.719807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 ls $extractedFolder\n",
    "!aws s3 rm --recursive $extractedFolder\n",
    "!aws s3 ls $extractedFolder\n",
    "!rm -f /efs/shared/regression/CREATOR_PASSED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.820194,
     "end_time": "2020-03-03T13:56:42.353994",
     "exception": false,
     "start_time": "2020-03-03T13:56:40.533800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 ls s3://$bucketName/$database_name/\n",
    "!aws s3 rm --recursive  s3://$bucketName/$database_name/\n",
    "!aws s3 ls s3://$bucketName/$database_name/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Lets Copy our Sample Data to our New s3 Bucket and Begin our Orchestration:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 9.978746,
     "end_time": "2020-03-03T13:56:53.033620",
     "exception": false,
     "start_time": "2020-03-03T13:56:43.054874",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "res=!orbit list env --variable=toolkitbucket\n",
    "dm_s3_bucket = res[0]\n",
    "print(dm_s3_bucket)\n",
    "!aws s3 sync s3://$dm_s3_bucket/data s3://$bucketName/landing/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://$dm_s3_bucket/cms/schema s3://$bucketName/landing/cms/schema/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schemas(source_bucket_name, prefix='', suffix=''):\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    bucket = s3.Bucket(name=source_bucket_name) \n",
    "    schemas = []\n",
    "    for o in bucket.objects.all():\n",
    "        if (o.key.startswith(prefix)):\n",
    "            name = os.path.basename(o.key).split(\".\")[0]\n",
    "            schemaStr = o.get()['Body'].read().decode('utf-8') \n",
    "            schema = json.loads(schemaStr) #StructType.fromJson(json.loads(schemaStr))\n",
    "            schemas.append((name, schema))\n",
    "    return schemas\n",
    "\n",
    "def get_schema(schemas, filename):\n",
    "    for (schema_name, schema) in schemas:\n",
    "        #print(f\"{schema_name} in {filename} : {schema_name in filename}\")\n",
    "        if schema_name in filename:\n",
    "            return schema_name, schema\n",
    "    return None, None\n",
    "\n",
    "schemas = get_schemas(bucketName, 'landing/cms/schema/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.085206,
     "end_time": "2020-03-03T13:56:53.220281",
     "exception": false,
     "start_time": "2020-03-03T13:56:53.135075",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***\n",
    "## Step 3: Extract Zip Files in Parallel\n",
    "\n",
    "We have completed all of the necessary set up and s3 clean up and our ready to move into the first phase of our data lake orchestration. Here we will:\n",
    "\n",
    "* Handle Zipped Files and Extract their CSV Data\n",
    "* Migrate the Extracted Data back to s3 in a new Target Bucket\n",
    "* Schedule Multiple Notebooks to Execute in Parallel\n",
    "\n",
    "We will schedule separate notebooks to run **Example-2-Extract-Files** and execute in parallel. You can refer to that notebook which goes step-by-step handling zipped files and unzipping and migrating their content back to s3.\n",
    "\n",
    "Here we will define error checking functions to assert that our extraction was successful. We ensure the number of executions matches what is expected and that there are no errors in our execution history:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.076462,
     "end_time": "2020-03-03T13:56:53.354324",
     "exception": false,
     "start_time": "2020-03-03T13:56:53.277862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_file_extraction():\n",
    "    notebooks = []\n",
    "    for key in s3.list_objects_v2(Bucket=bucketName, Prefix=sourceFolder)['Contents']:\n",
    "        file = key['Key']\n",
    "        schema = get_schema(schemas, file)\n",
    "        s3_data_folder = os.path.join(extractedFolder, schema[0] if schema[0] else \"\")\n",
    "        notebook = {\n",
    "          \"notebookName\": \"Example-2-Extract-Files.ipynb\",\n",
    "          \"sourcePath\": \"/efs/shared/samples/notebooks/A-LakeCreator\",\n",
    "          \"targetPath\": \"/efs/shared/regression/notebooks/A-LakeCreator\",\n",
    "          \"params\": {\n",
    "            \"bucketName\": bucketName,\n",
    "            \"zipFileName\": file,\n",
    "            \"targetFolder\": s3_data_folder,\n",
    "            \"use_subdirs\" : False if schema[0] else True\n",
    "          },\n",
    "        }\n",
    "        notebooks.append(notebook)\n",
    "\n",
    "    notebooksToRun = {\n",
    "      \"compute\": {\n",
    "          \"container\" : {\n",
    "              \"p_concurrent\": \"10\"\n",
    "          },\n",
    "          \"node_type\": \"ec2\"\n",
    "      },\n",
    "      \"tasks\":  notebooks  \n",
    "    }\n",
    "    # notebooks\n",
    "    containers = controller.run_notebooks(notebooksToRun)\n",
    "    print (containers)\n",
    "    controller.wait_for_tasks_to_complete([containers], 60,10, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractedFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkNotebooks(executions, expected_count):\n",
    "    assert len(executions) == expected_count\n",
    "    for index, row in executions.iterrows():\n",
    "        if 'error@' in row['relativePath']:\n",
    "            raise AssertionError('error in ' + row['relativePath'])\n",
    "    print(\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 181.381817,
     "end_time": "2020-03-03T13:59:54.825898",
     "exception": false,
     "start_time": "2020-03-03T13:56:53.444081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "run_file_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get job -n lake-creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.150596,
     "end_time": "2020-03-03T13:59:55.082921",
     "exception": false,
     "start_time": "2020-03-03T13:59:54.932325",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "executions = controller.get_execution_history(\"/efs/shared/regression/notebooks/A-LakeCreator\", \"Example-2-Extract-Files.ipynb\")\n",
    "executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.089923,
     "end_time": "2020-03-03T13:59:55.296783",
     "exception": false,
     "start_time": "2020-03-03T13:59:55.206860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    checkNotebooks(executions, 8)\n",
    "except AssertionError as e:\n",
    "    print(\"Failed once, lets give one more try\")\n",
    "    !rm -rf /efs/shared/regression/notebooks/A-LakeCreator/Example-2*\n",
    "    run_file_extraction()\n",
    "    executions = controller.get_execution_history(\"/efs/shared/regression/notebooks/A-LakeCreator\", \"Example-2-Extract-Files.ipynb\")\n",
    "    checkNotebooks(executions,8)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***We will check our Output in our extractedFolder and split the output into an array of File Outputs:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.537052,
     "end_time": "2020-03-03T13:59:55.946789",
     "exception": false,
     "start_time": "2020-03-03T13:59:55.409737",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash --out output --err error -s $extractedFolder\n",
    "\n",
    "aws s3 ls \"$1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.089318,
     "end_time": "2020-03-03T13:59:56.097813",
     "exception": false,
     "start_time": "2020-03-03T13:59:56.008495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(output)\n",
    "files = output.split('\\n')\n",
    "print(\"total files: \" + str(len(files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.057628,
     "end_time": "2020-03-03T13:59:56.212646",
     "exception": false,
     "start_time": "2020-03-03T13:59:56.155018",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "***\n",
    "## Step 4: Read the CSV Files and Create Glue tables with Parquet format according to schema\n",
    "\n",
    "Our Zipped files have been succesfully extracted and their CSV data content is placed in our **extractedFolder**. We now must collect the schema for our different data tables saved as csv files and create Parquet Output Glue Tables with set schema located in our target directory on s3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Glue Tables\n",
    "We will now create our Parquet formatted Glue Tables by scheduling and executing the notebook, **Example-3-Load-Database-Athena**. For each extracted csv data file in our extracted folder our notebook will perform the following:\n",
    "\n",
    "* Find for each file the corresponding schema\n",
    "\n",
    "* Read the file using spark\n",
    "\n",
    "* Create external tables to create the Glue table and parquet output\n",
    "\n",
    "You can refer to that notebook which goes step-by-step through the process of schema detection and table creation. We will execute the **run_glue_table_loading** on 4 concurrent containers and checks the execution history to ensure that the code ran for all the data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.118782,
     "end_time": "2020-03-03T14:06:18.334301",
     "exception": false,
     "start_time": "2020-03-03T14:06:18.215519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_data_notebook = \"Example-3-Load-Database-Athena\"\n",
    "\n",
    "def run_glue_table_loading(concurrent_containers=4):\n",
    "    containers = []\n",
    "    found_schemas = []\n",
    "    i = 0\n",
    "    for key in s3.list_objects_v2(Bucket=bucketName, Prefix=extractedPrefix)['Contents']:\n",
    "        file = key['Key']\n",
    "        p = Path(file).parent\n",
    "        schema = get_schema(schemas, file)\n",
    "        if schema in found_schemas:\n",
    "            continue\n",
    "        i = i + 1\n",
    "        print(f\"Found schema: {schema[0]}\")\n",
    "        found_schemas.append(schema)\n",
    "        notebooksToRun = {\n",
    "          \"compute\": {\n",
    "              \"node_type\": \"ec2\",\n",
    "              \"container\": {\n",
    "                  \"p_concurrent\" :1\n",
    "              }\n",
    "          },\n",
    "          \"tasks\":  [\n",
    "                {\n",
    "                      \"notebookName\": f\"{load_data_notebook}.ipynb\",\n",
    "                      \"sourcePath\": \"/efs/shared/samples/notebooks/A-LakeCreator\",\n",
    "                      \"targetPath\": \"/efs/shared/regression/notebooks/A-LakeCreator\",\n",
    "                      \"targetPrefix\": \"unsecured-{}\".format(i),\n",
    "                      \"params\": {\n",
    "                            \"source_bucket_name\" : bucketName,\n",
    "                            \"target_bucket_name\" : bucketName,\n",
    "                            \"database_name\" : \"cms_raw_db\",\n",
    "                            \"schema_dir\" : \"landing/cms/schema\",\n",
    "                            \"file_path\": str(p),\n",
    "                            \"region\": my_region\n",
    "                      }      \n",
    "                }\n",
    "          ],\n",
    "          \"env_vars\": [\n",
    "                {\n",
    "                    'name': 'AWS_ORBIT_S3_BUCKET',\n",
    "                    'value': bucketName\n",
    "                }\n",
    "          ]\n",
    "        }\n",
    "\n",
    "        t = time.localtime()\n",
    "        current_time = time.strftime(\"%H:%M:%S\", t)\n",
    "\n",
    "        container = controller.run_notebooks(notebooksToRun)\n",
    "        containers.append(container)\n",
    "        print(\"task : \", current_time , str(container), \"-->\", notebooksToRun['tasks'][0]['params']['file_path'])\n",
    "        if i%concurrent_containers == 0:\n",
    "            print(f\"Now waiting for {str(len(containers))} tasks to complete before spawning new ones\")\n",
    "            controller.wait_for_tasks_to_complete(containers, 60,30, False)\n",
    "            print(\"task : \", containers, \" done \")\n",
    "            containers = []\n",
    "    \n",
    "    if len(containers) > 0: \n",
    "        print(f\"Now waiting for {str(len(containers))} tasks to complete before spawning new ones\")\n",
    "        controller.wait_for_tasks_to_complete(containers, 60,15, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 848.4511,
     "end_time": "2020-03-03T14:20:26.843545",
     "exception": false,
     "start_time": "2020-03-03T14:06:18.392445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "concurrent_containers = 10\n",
    "for retry in range(0, 3):\n",
    "    try: \n",
    "        !rm -rf /efs/shared/regression/notebooks/A-LakeCreator/$load_data_notebook/*\n",
    "        run_glue_table_loading(concurrent_containers)\n",
    "        executions = controller.get_execution_history(\"/efs/shared/regression/notebooks/A-LakeCreator\", f\"{load_data_notebook}.ipynb\")  \n",
    "        display(executions)\n",
    "        checkNotebooks(executions,5)\n",
    "        break\n",
    "    except AssertionError as e:\n",
    "        print(f\"Failed {retry}, lets give one more try\")\n",
    "        #concurrent_containers = concurrent_containers - 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.073176,
     "end_time": "2020-03-03T14:20:27.040395",
     "exception": false,
     "start_time": "2020-03-03T14:20:26.967219",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Check that Glue Tables are Created\n",
    "We will ensure that we now have 7 tables created from our 7 data files that we extracted earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.195756,
     "end_time": "2020-03-03T14:20:27.347556",
     "exception": false,
     "start_time": "2020-03-03T14:20:27.151800",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "glue = boto3.client('glue')\n",
    "res = glue.get_tables(DatabaseName='cms_raw_db')\n",
    "tables = res['TableList']\n",
    "raw_count = 0\n",
    "parq_count = 0\n",
    "for t in tables:\n",
    "    if t['Name'].endswith('_raw'):\n",
    "        raw_count += 1\n",
    "    else:\n",
    "        parq_count += 1\n",
    "    print(t['Name'])\n",
    "print(f\"Total tables: {str(len(tables))}. Raw tables: {raw_count}. Final tables: {parq_count}\")\n",
    "assert raw_count == parq_count and raw_count > 0\n",
    "!echo \"PASSED\" >> /efs/shared/regression/CREATOR_PASSED\n",
    "!ls /efs/shared/regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get job -n lake-creator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.068896,
     "end_time": "2020-03-03T14:20:28.275511",
     "exception": false,
     "start_time": "2020-03-03T14:20:28.206615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "# End of notebook\n",
    "\n",
    "**Congratulations! You have just built you very own Data Lake with countless Amazon Web Service integrations through AWS Orbit Workbench**\n",
    "\n",
    "<img style=\"width:100px;height:100px;border:0;\" src=\"https://images-na.ssl-images-amazon.com/images/I/71pHyDfdXwL._SL1500_.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "papermill": {
   "duration": 1433.197715,
   "end_time": "2020-03-03T14:20:28.746673",
   "environment_variables": {},
   "exception": null,
   "input_path": "/tmp/e1@20200303-13:56.ipynb",
   "output_path": "s3://orbit-test-base-acco-testnotebookbucket29e9e4-g80prenjos07/lake-creator/output/notebooks/tests/A-LakeCreator/Example-1-Build-Lake/e1@20200303-13:56.ipynb",
   "parameters": {
    "PAPERMILL_INPUT_PATH": "/tmp/e1@20200303-13:56.ipynb",
    "PAPERMILL_OUTPUT_DIR_PATH": "s3://orbit-test-base-acco-testnotebookbucket29e9e4-g80prenjos07/lake-creator/output/notebooks/tests/A-LakeCreator/Example-1-Build-Lake",
    "PAPERMILL_OUTPUT_PATH": "s3://orbit-test-base-acco-testnotebookbucket29e9e4-g80prenjos07/lake-creator/output/notebooks/tests/A-LakeCreator/Example-1-Build-Lake/e1@20200303-13:56.ipynb",
    "PAPERMILL_WORKBOOK_NAME": "e1@20200303-13:56.ipynb",
    "PAPERMILL_WORK_DIR": "/ws/ScienceRepo1/samples/notebooks/A-LakeCreator"
   },
   "start_time": "2020-03-03T13:56:35.548958",
   "version": "1.2.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}