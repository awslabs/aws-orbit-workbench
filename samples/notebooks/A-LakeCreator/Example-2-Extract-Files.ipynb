{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "#  Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "#\n",
    "#    Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "#    You may not use this file except in compliance with the License.\n",
    "#    You may obtain a copy of the License at\n",
    "#\n",
    "#        http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#    Unless required by applicable law or agreed to in writing, software\n",
    "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#    See the License for the specific language governing permissions and\n",
    "#    limitations under the License.\n",
    "-->\n",
    "\n",
    "# Extracting files in parallel using notebooks\n",
    "***Extracting Data from Zipped Files and Migrating Output Files to S3 Buckets***\n",
    "\n",
    "___\n",
    "---\n",
    "## Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup and Define Parameters](#Setup-and-Define-Parameters)\n",
    " 1. [Define Parameters](#Define-Parameters)\n",
    " 2. [Copy Zip Files Locally to Handle Extraction](#Copy-Zip-Files-Locally-to-Handle-Extraction)\n",
    "3. [Extract Zip Files](#Step-2:-Extract-Zip-Files)\n",
    "4. [Check For Errors and Clean Up](#Step-3:-Checking-for-Errors-and-Clean-Up)\n",
    "  \n",
    "---\n",
    "\n",
    "## Introduction\n",
    "This notebook goes through the process of extracting zip files and migrating their unzipped content to s3. We will go through the following steps to extract our files:\n",
    "1. Migrate the files to our local environment\n",
    "2. Use Shell Commands in our notebook with IPython to unzip the files\n",
    "3. Use AWS CLI Commands to move these unzipepd files to a remote target s3 bucket. \n",
    "\n",
    "When calling on this notebook to run in **Example 1: \"Orchestration Notebook for Building the Lake\"**, we will concurrently notebooks for each of the different zipped files on AWS Fargate which provides a serverless container execution environment. This way we can reduce time and extract our zip files in parallel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Setup\n",
    "\n",
    "#### Define Parameters \n",
    "\n",
    "First, let's define the source folder, s3 bucket path, and zip file name for our zip files we wish to extract. This will allow us to format an extract path where the Zip file sits in our remote environment\n",
    "\n",
    "We also will specify an s3 bucket path for our target folders which will be where the extracted contnet will be placed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "sourceFolder = \"landing/\"\n",
    "bucketName = \"orbit-test-base-accoun-testlakebucketfa111111-1111111111\"\n",
    "zipFileName = \"landing/cms/DE1_0_2008_Beneficiary_Summary_File_Sample_1.zip\"\n",
    "targetFolder = \"s3://orbit-test-base-accoun-testlakebucketfa111111-1111111111/extracted/\"\n",
    "use_subdirs = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toExtractPath = \"s3://{}/{}\".format(bucketName,zipFileName)\n",
    "\n",
    "toExtractPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Copy Zip Files Locally to Handle Extraction\n",
    "Once we have defined our parameters we can copy the zip files over from our s3 bucket \"ExtractPath\" to a zip file located on our local environment. This will allow us to call on the shell commands to unzip our file and move it back to cloud storage in s3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $toExtractPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $toExtractPath ./$zipFileName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Here we are just removing the filename extension so we can store unzipped content in the same named file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseName = zipFileName.split(\".\")[0]\n",
    "baseName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Extract Zip Files\n",
    "Now, let's call on the **unzip Shell command** to unzip our file in our local source location and transfer the unzipped file to the target directory \"baseName\".\n",
    "\n",
    "We will then check that we have a valid target Folder name in s3 to move the unzipped content back to cloud storage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fR ./$baseName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!unzip ./$zipFileName -d ./$baseName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_subdirs:\n",
    "    filename = baseName.split(\"/\")[-1]\n",
    "    targetFolder += filename\n",
    "targetFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move Output and Error Files to Target s3 Bucket(s)\n",
    "Lastly, let's use \"**%%bash script magics**\" to run cells with bash in a subprocess. We can copy all of the output and errors (if any) to our target folder in s3 to complete the extraction process for our zip files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --out output --err error -s \"$baseName\" \"$targetFolder\"\n",
    "echo \"aws s3 cp --recursive ./$1 $2\"\n",
    "aws s3 cp --recursive ./$1 $2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Checking for Errors and Clean Up\n",
    "Lets double check that we did not run into any errors during the process unzipping our zip files. We can check to see if any errors were logged when unzipping and assert that no errors were found if successful. \n",
    "\n",
    "Next, we can remove our two local directories holding our zipped file and our unzipped file(s) and continue building out Data Lake with our unzipped data securely stored in s3:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)\n",
    "print(error)\n",
    "assert \"upload\" in output\n",
    "assert len(error) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fR ./$baseName\n",
    "!rm -f ./$zipFileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}