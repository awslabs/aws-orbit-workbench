{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.04876,
     "end_time": "2020-02-29T17:44:03.296020",
     "exception": false,
     "start_time": "2020-02-29T17:44:03.247260",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<!--\n",
    "#  Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "#\n",
    "#    Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "#    You may not use this file except in compliance with the License.\n",
    "#    You may obtain a copy of the License at\n",
    "#\n",
    "#        http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#    Unless required by applicable law or agreed to in writing, software\n",
    "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#    See the License for the specific language governing permissions and\n",
    "#    limitations under the License.\n",
    "-->\n",
    "\n",
    "# Prepare the Data Lake from Raw CSV Files\n",
    "***Determine Data Schema and Create Parquet Output with Glue Metadata***\n",
    "\n",
    "---\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Set Up](#Set-Up)\n",
    "3. [Get Schema for Files](#Get-Schema-for-Files)\n",
    " 1. [Add Helper Functions to Configure Schema Output](#Add-Helper-Functions-to-Configure-Schema-Output)\n",
    " 2. [Get File Schema](#Get-File-Schema)\n",
    "4. [Create External Table with Parquet Output](#Create-External-Table-with-Parquet-Output)\n",
    "5. [Produce Loading Statistics](#Produce-Loading-Statistics)\n",
    "6. [Final Check](#Final-Check)\n",
    "---\n",
    "## Introduction\n",
    "In this notebook we go through the process of finding the schema for our data files, and creating Parquet output tables with set schema located in our target directory on s3. The notebook integrates Amazon services, such as Athena and S3. \n",
    "\n",
    "In summary, this job will perform the following functions:\n",
    "\n",
    "**1.** Find for each file the corresponding schema \n",
    "\n",
    "**2.** Read the file using Athena external table\n",
    "\n",
    "**3.** Create final tables in with parquet output format, and transform \"date\" fields from string into type `date`\n",
    "\n",
    "___\n",
    "\n",
    "### Author: AWS Professional Services Emerging Technology and Intelligent Platforms Group\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Set Up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.246107,
     "end_time": "2020-02-29T17:44:03.585682",
     "exception": false,
     "start_time": "2020-02-29T17:44:03.339575",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import json\n",
    "from aws_orbit_sdk.database import get_athena\n",
    "from aws_orbit_sdk.common import get_workspace,get_scratch_database\n",
    "# import aws_orbit_sdk.glue as orbit_catalog_api\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import glob\n",
    "import tempfile\n",
    "import shutil\n",
    "from time import time, sleep\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.174656,
     "end_time": "2020-02-29T17:44:30.415375",
     "exception": false,
     "start_time": "2020-02-29T17:44:30.240719",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Define constants\n",
    "batch = '1'\n",
    "database_name = \"cms_raw_db\"\n",
    "source_bucket_name = \"orbit-test-base-accoun-testlakebucketfa111111-1111111111\"\n",
    "target_bucket_name = \"orbit-test-base-accoun-testlakebucketfa111111-1111111111\"\n",
    "schema_dir = \"landing/cms/schema\"\n",
    "file_path = 'extracted/Beneficiary_Summary'\n",
    "region = \"us-east-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.114365,
     "end_time": "2020-02-29T17:44:30.951902",
     "exception": false,
     "start_time": "2020-02-29T17:44:30.837537",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_db_dir = database_name + \"/\"\n",
    "incoming_dir = file_path\n",
    "basename = Path(file_path).name\n",
    "workspace = get_workspace()\n",
    "team_space = workspace['team_space']\n",
    "scratch_bucket = f\"{workspace['ScratchBucket']}/{team_space}\"\n",
    "# workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Some helping python functions to work with services: S3, Athena:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WAIT_RETRY_DELAY = 1 # seconds. Amount of time to wait before re-checking the status of the query that is being executed ini Athena.\n",
    "\n",
    "def getSchemas(source_bucket_name, prefix='', suffix=''):\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    bucket = s3.Bucket(name=source_bucket_name) \n",
    "    schemas = []\n",
    "    for o in bucket.objects.all():\n",
    "        if (o.key.startswith(prefix)):\n",
    "            name = os.path.basename(o.key).split(\".\")[0]\n",
    "            schemaStr = o.get()['Body'].read().decode('utf-8') \n",
    "            schema = json.loads(schemaStr) #StructType.fromJson(json.loads(schemaStr))\n",
    "            schemas.append((name, schema))\n",
    "    return schemas\n",
    "\n",
    "def getSchema(schemas, filename):\n",
    "    for (schema_name, schema) in schemas:\n",
    "        #print(f\"{schema_name} in {filename} : {schema_name in filename}\")\n",
    "        if schema_name in filename:\n",
    "            return schema_name, schema\n",
    "    return None, None\n",
    "\n",
    "CREATE_CSV_TABLE = \"\"\"\n",
    "        CREATE EXTERNAL TABLE {}.{}\n",
    "        ({})\n",
    "        row format delimited\n",
    "        fields terminated by ',' \n",
    "        LOCATION '{}'                      \n",
    "        tblproperties (\"skip.header.line.count\"=\"1\")\n",
    "\"\"\"\n",
    "def schema_2_ddl(schema, database_name, table_name, s3_target_dir):\n",
    "    if not schema or not schema.get(\"fields\"):\n",
    "        return None\n",
    "    columns_def_arr = [f\"{column['name']} {map_column_type(column['type'])}\" for column in schema[\"fields\"]]\n",
    "    columns_def = \",\".join(columns_def_arr)\n",
    "    s3_target_location = \"s3://{}/{}/\".format(target_bucket_name, s3_target_dir)\n",
    "    return CREATE_CSV_TABLE.format(database_name, table_name, columns_def, s3_target_location)\n",
    "\n",
    "def map_column_type(schema_type):\n",
    "    \"\"\"Returns adopted type for athena\"\"\"\n",
    "    if schema_type == 'date':\n",
    "        return \"string\"\n",
    "    if schema_type == 'integer':\n",
    "        return \"int\"\n",
    "    if schema_type == 'long':\n",
    "        return \"bigint\"\n",
    "    \n",
    "    return schema_type\n",
    "\n",
    "\n",
    "def delete_temp_tbl(athena_client):    \n",
    "    TIMEOUT = 30 # seconds\n",
    "    start = time()\n",
    "    success_cd = False\n",
    "    response = athena_client.start_query_execution(\n",
    "            QueryString = f\"DROP TABLE IF EXISTS {database_name}.{basename}_raw\",\n",
    "            ResultConfiguration = {'OutputLocation':  f's3://{scratch_bucket}/athena/results'}\n",
    "    )\n",
    "    query_id = response['QueryExecutionId']\n",
    "    while (time() - start < TIMEOUT):\n",
    "        response = athena_client.get_query_execution(QueryExecutionId=query_id)\n",
    "        if response['QueryExecution']['Status']['State'] in ['SUCCEEDED']:\n",
    "            success_cd = True\n",
    "            break\n",
    "        elif response['QueryExecution']['Status']['State'] in ['FAILED', 'CANCELLED']:\n",
    "            break\n",
    "        sleep(WAIT_RETRY_DELAY)\n",
    "\n",
    "    if not success_cd:\n",
    "        raise Exception (f\"FAILED to execute DDL: DROP TABLE IF EXISTS {database_name}.{basename}_raw\")\n",
    "\n",
    "    \n",
    "\n",
    "def createTempExternalTable(athena_client, ddl):\n",
    "    TIMEOUT = 30 # seconds\n",
    "    start = time()\n",
    "    success_cd = False\n",
    "    response = athena_client.start_query_execution(\n",
    "            QueryString = ddl,\n",
    "            ResultConfiguration = {'OutputLocation':  f's3://{scratch_bucket}/athena/results'}\n",
    "    )\n",
    "    query_id = response['QueryExecutionId']\n",
    "    while (time() - start < TIMEOUT):\n",
    "        response = athena_client.get_query_execution(QueryExecutionId=query_id)\n",
    "        if response['QueryExecution']['Status']['State'] in ['SUCCEEDED']:\n",
    "            success_cd = True\n",
    "            break\n",
    "        elif response['QueryExecution']['Status']['State'] in ['FAILED', 'CANCELLED']:\n",
    "            break\n",
    "        sleep(WAIT_RETRY_DELAY)\n",
    "\n",
    "    if not success_cd:\n",
    "        raise Exception (f\"FAILED to execute DDL: {ddl}\")\n",
    "\n",
    "CREATE_PARQUET_TABLE = \"\"\"\n",
    "        CREATE TABLE {}.{}\n",
    "        WITH (\n",
    "            format = 'Parquet',\n",
    "            parquet_compression = 'SNAPPY',\n",
    "            external_location = '{}'\n",
    "        )\n",
    "        AS\n",
    "        (select {} from {}.{})\n",
    "\"\"\"\n",
    "def loadTable(athena_client, schema_name, schema):\n",
    "    \"\"\" Uses CTAS to load data from temp table into a target one\"\"\"\n",
    "    if not schema or not schema.get(\"fields\"):\n",
    "        return None\n",
    "\n",
    "    \n",
    "    TIMEOUT = 30 # seconds\n",
    "    start = time()\n",
    "    success_cd = False\n",
    "    response = athena_client.start_query_execution(\n",
    "            QueryString = f\"DROP TABLE IF EXISTS {database_name}.{basename}\",\n",
    "            ResultConfiguration = {'OutputLocation':  f's3://{scratch_bucket}/athena/results'}\n",
    "    )\n",
    "    query_id = response['QueryExecutionId']\n",
    "    while (time() - start < TIMEOUT):\n",
    "        response = athena_client.get_query_execution(QueryExecutionId=query_id)\n",
    "        if response['QueryExecution']['Status']['State'] in ['SUCCEEDED']:\n",
    "            success_cd = True\n",
    "            break\n",
    "        elif response['QueryExecution']['Status']['State'] in ['FAILED', 'CANCELLED']:\n",
    "            break\n",
    "        sleep(WAIT_RETRY_DELAY)\n",
    "\n",
    "    if not success_cd:\n",
    "        raise Exception (f\"FAILED to execute DDL: DROP TABLE IF EXISTS {database_name}.{basename}\")\n",
    "\n",
    "    try:\n",
    "        s3 = boto3.resource('s3')\n",
    "        bucket = s3.Bucket(target_bucket_name)\n",
    "        bucket.objects.filter(Prefix=f\"{target_db_dir}{basename}/\").delete()        \n",
    "    except Exception as e:\n",
    "        print(\"Failed with \" + str(e))\n",
    "    \n",
    "    \n",
    "    columns_def_arr = []\n",
    "    for column in schema[\"fields\"]:\n",
    "        if column['type']!='date':\n",
    "            columns_def_arr.append(column['name'])\n",
    "        else:\n",
    "            columns_def_arr.append(f\"case when {column['name']} is not null and {column['name']}!= '' then date(parse_datetime({column['name']}, 'yyyyMMdd')) else null end {column['name']}\")\n",
    "    columns_def = \", \\n\".join(columns_def_arr)\n",
    "    \n",
    "    ddl = CREATE_PARQUET_TABLE.format(\n",
    "                database_name, basename,\n",
    "                \"s3://{}/{}{}/\".format(target_bucket_name, target_db_dir, basename),\n",
    "                columns_def,\n",
    "                database_name,\n",
    "                basename+\"_raw\"\n",
    "            )\n",
    "    TIMEOUT = 30 # seconds\n",
    "    start = time()\n",
    "    success_cd = False\n",
    "    response = athena_client.start_query_execution(\n",
    "            QueryString = ddl,\n",
    "            ResultConfiguration = {'OutputLocation':  f's3://{scratch_bucket}/athena/results'}\n",
    "    )\n",
    "    query_id = response['QueryExecutionId']\n",
    "    while (time() - start < TIMEOUT):\n",
    "        response = athena_client.get_query_execution(QueryExecutionId=query_id)\n",
    "        if response['QueryExecution']['Status']['State'] in ['SUCCEEDED']:\n",
    "            success_cd = True\n",
    "            break\n",
    "        elif response['QueryExecution']['Status']['State'] in ['FAILED', 'CANCELLED']:\n",
    "            break\n",
    "        sleep(WAIT_RETRY_DELAY)\n",
    "\n",
    "    if not success_cd:\n",
    "        raise Exception (f\"FAILED to execute DDL: {ddl}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "schemas = getSchemas(source_bucket_name, schema_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Time to run some code and see results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all schemas from the S3 location\n",
    "schemas = getSchemas(source_bucket_name, schema_dir)\n",
    "# Find a schema for given file\n",
    "schema_name, schema = getSchema(schemas, basename)\n",
    "# Convert schema to DDL for temp table\n",
    "ddl = schema_2_ddl(schema, database_name, basename+\"_raw\", file_path) \n",
    "\n",
    "athena_client = boto3.client(\"athena\")\n",
    "\n",
    "delete_temp_tbl(athena_client)\n",
    "createTempExternalTable(athena_client, ddl)\n",
    "\n",
    "loadTable(athena_client, schema_name, schema)\n",
    "# delete_temp_tbl(athena_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena = get_athena() # now, let's use SQL magic with Athena\n",
    "\n",
    "%config SqlMagic.autocommit=False # for engines that do not support autommit\n",
    "\n",
    "workspace = get_workspace()\n",
    "# scratch_glue_db = get_scratch_database()\n",
    "team_space = workspace['team_space']\n",
    "workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_db = \"cms_raw_db\"\n",
    "target_db = \"users\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%connect_to_athena -database $glue_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Quick Check:** Ensuring that all of our Extracted Data is in our Database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "SELECT 1 as \"Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 9.637008,
     "end_time": "2020-02-29T17:44:46.307097",
     "exception": false,
     "start_time": "2020-02-29T17:44:36.670089",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "-- # %%spark -s spark -c sql \n",
    "SHOW TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading data from table {glue_db}.{basename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "query = f\"SELECT * FROM {glue_db}.{basename} limit 13\"\n",
    "ds = %sql $query\n",
    "df = ds.DataFrame()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data load complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.618185,
     "end_time": "2020-02-29T17:45:38.526811",
     "exception": false,
     "start_time": "2020-02-29T17:45:37.908626",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## Final Check\n",
    "Let's run two final checks on our loading stats:\n",
    "\n",
    " **1.** The count of the columns is greater than 0\n",
    " \n",
    " **2.** The count of rows is grreater than 0\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.579421,
     "end_time": "2020-02-29T17:45:39.695594",
     "exception": false,
     "start_time": "2020-02-29T17:45:39.116173",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert len(df.columns) > 0\n",
    "\n",
    "assert len(df.index) > 0 # number of rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "papermill": {
   "duration": 126.950722,
   "end_time": "2020-02-29T17:46:08.022565",
   "environment_variables": {},
   "exception": null,
   "input_path": "/tmp/unsecured-1@20200229-17:44.ipynb",
   "output_path": "s3://orbit-test-base-acco-testnotebookbucket29e9e4-9l20vrej4ro/lake-creator/output/notebooks/tests/A-LakeCreator/Example-3-Load-Database/unsecured-1@20200229-17:44.ipynb",
   "parameters": {
    "PAPERMILL_INPUT_PATH": "/tmp/unsecured-1@20200229-17:44.ipynb",
    "PAPERMILL_OUTPUT_DIR_PATH": "s3://orbit-test-base-acco-testnotebookbucket29e9e4-9l20vrej4ro/lake-creator/output/notebooks/tests/A-LakeCreator/Example-3-Load-Database",
    "PAPERMILL_OUTPUT_PATH": "s3://orbit-test-base-acco-testnotebookbucket29e9e4-9l20vrej4ro/lake-creator/output/notebooks/tests/A-LakeCreator/Example-3-Load-Database/unsecured-1@20200229-17:44.ipynb",
    "PAPERMILL_WORKBOOK_NAME": "unsecured-1@20200229-17:44.ipynb",
    "PAPERMILL_WORK_DIR": "/ws/ScienceRepo1/samples/notebooks/A-LakeCreator",
    "batch": "1",
    "database_name": "cms_raw_db",
    "file_path": "extracted/DE1_0_2008_Beneficiary_Summary_File_Sample_1.csv",
    "region": "us-east-1",
    "schema_dir": "landing/cms/schema",
    "source_bucket_name": "orbit-test-base-accoun-testlakebucketfa17f7f5-43nn7fk3hx5c",
    "target_bucket_name": "orbit-test-base-accoun-testlakebucketfa17f7f5-43nn7fk3hx5c"
   },
   "start_time": "2020-02-29T17:44:01.071843",
   "version": "1.2.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}